{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR 4.3, 5.2 en 5.3\n",
    "Van Pjotr en Sennen\n",
    "\n",
    "Hierin gaan wij een paar queries aanvragen aan de database die wij hebben gemaakt gebasseerd op de ETL diagram van de Great_Outdoors.\n",
    "\n",
    "Hieronder zullen we beginnen met de setup van de libraries en het verbinden met de database. We zullen de pyodbc library gebruiken om de verbinding tussen SSMS en Python. Verder gebruiken we ook pandas om data makkelijk te lezen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Cursor object at 0x000001743582AFB0>\n"
     ]
    }
   ],
   "source": [
    "DB = {'servername': os.getenv('NAME'),\n",
    "      'database': os.getenv('DATABASE'),\n",
    "      'username': os.getenv('USER'),\n",
    "      'password': os.getenv('PASSWORD')}\n",
    "\n",
    "# Increase the connection timeout value to 30 seconds\n",
    "conn_str = f\"DRIVER=SQL Server;SERVER={DB['servername']};DATABASE={DB['database']};UID={DB['username']};PWD={DB['password']};Trusted_Connection=yes;Connection\"\n",
    "\n",
    "conn = pyodbc.connect(conn_str, timeout=120)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "engine = sqlalchemy.create_engine(f\"mssql+pyodbc:///?odbc_connect={conn_str}\")\n",
    "\n",
    "# Hoe checken we of de connectie werkt?\n",
    "print(cursor.execute(\"SELECT @@version;\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om ervoor te zorgen dat we weten dat we data kunnen aanvragen vanaf de server zullen we hier een paar queries executeren. Zodat we kunnen bevestigen dat we alles goed hebben geconfigureerd. Eerst pakken we alle tabelen van uit de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCT\n",
      "SALES_STAFF\n",
      "SATISFACTION_TYPE\n",
      "COURSE\n",
      "PRODUCT_NUMBER\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT t.name FROM sys.tables t\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Voor elke tabel in de database print de naam van de tabel\n",
    "if(tables == []):\n",
    "    print(\"No tables found, the database is empty.\")\n",
    "else:\n",
    "    for table in tables:\n",
    "        table = table[0]\n",
    "        print(table)\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        cursor.execute(f\"DROP TABLE {table_name}\")\n",
    "        conn.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder checken we of we alle drivers hebben geinstalleerd. Meestal maken we gebruikt van de SQL Server driver en SQLite driver. Maar als je op een nieuwer systeem zit kan je ook gebruik maken van de Microsoft Access driver gebruik maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SQL Server',\n",
       " 'MySQL ODBC 8.0 ANSI Driver',\n",
       " 'MySQL ODBC 8.0 Unicode Driver',\n",
       " 'SQL Server Native Client RDA 11.0',\n",
       " 'ODBC Driver 17 for SQL Server',\n",
       " 'Microsoft Access Driver (*.mdb, *.accdb)',\n",
       " 'Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)',\n",
       " 'Microsoft Access Text Driver (*.txt, *.csv)']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daarna wat we willen zijn dus de functies maken die we later in het project gaan gebruiken om de data in te laden in ons project. Zodat we de data uit de bron kunnen transformeren en overzetten naar onze SQL Server database. We maken hiervoor een extract functie maken en een laad functie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "Eerst gaan we de data uit de access database halen en ervoor zorgen dat we ze later goed kunnen transformeren in de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import sales\n",
      "Imported staff\n",
      "Imported crm tables\n",
      "Imported inventory\n",
      "Imported sales_product_forecast\n"
     ]
    }
   ],
   "source": [
    "select_tables = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "\n",
    "# Verbind met sqlite go_sales staff\n",
    "sales_conn = sqlite3.connect(\"go_sales.sqlite\")\n",
    "sales_tables = pd.read_sql_query(select_tables, sales_conn)\n",
    "\n",
    "sales_country       = pd.read_sql_query(\"SELECT * FROM country;\", sales_conn)\n",
    "order_details       = pd.read_sql_query(\"SELECT * FROM order_details;\", sales_conn)\n",
    "order_header        = pd.read_sql_query(\"SELECT * FROM order_header;\", sales_conn)\n",
    "order_method        = pd.read_sql_query(\"SELECT * FROM order_method;\", sales_conn)\n",
    "product             = pd.read_sql_query(\"SELECT * FROM product;\", sales_conn)\n",
    "product_line        = pd.read_sql_query(\"SELECT * FROM product_line;\", sales_conn)\n",
    "product_type        = pd.read_sql_query(\"SELECT * FROM product_type;\", sales_conn)\n",
    "sales_retailer_site = pd.read_sql_query(\"SELECT * FROM retailer_site;\", sales_conn)\n",
    "return_reason       = pd.read_sql_query(\"SELECT * FROM return_reason;\", sales_conn)\n",
    "returned_item       = pd.read_sql_query(\"SELECT * FROM returned_item;\", sales_conn)\n",
    "sales_branch        = pd.read_sql_query(\"SELECT * FROM sales_branch;\", sales_conn)\n",
    "sales_staff         = pd.read_sql_query(\"SELECT * FROM sales_staff;\", sales_conn)\n",
    "SALES_TARGETData    = pd.read_sql_query(\"SELECT * FROM SALES_TARGETData;\", sales_conn)\n",
    "sqlite_sequence     = pd.read_sql_query(\"SELECT * FROM sqlite_sequence;\", sales_conn)\n",
    "print(\"Import sales\")\n",
    "\n",
    "staff_conn = sqlite3.connect(\"go_staff.sqlite\")\n",
    "staff_tables = pd.read_sql_query(select_tables, staff_conn)\n",
    "course            = pd.read_sql_query(\"SELECT * FROM course;\", staff_conn)\n",
    "sales_branch      = pd.read_sql_query(\"SELECT * FROM sales_branch;\", staff_conn)\n",
    "sales_staff       = pd.read_sql_query(\"SELECT * FROM sales_staff;\", staff_conn)\n",
    "satisfaction      = pd.read_sql_query(\"SELECT * FROM satisfaction;\", staff_conn)\n",
    "satisfaction_type = pd.read_sql_query(\"SELECT * FROM satisfaction_type;\", staff_conn)\n",
    "training          = pd.read_sql_query(\"SELECT * FROM training;\", staff_conn)\n",
    "print(\"Imported staff\")\n",
    "\n",
    "crm_conn = sqlite3.connect(\"go_crm.sqlite\")\n",
    "crm_tables = pd.read_sql_query(select_tables, crm_conn)\n",
    "                           \n",
    "age_group             = pd.read_sql_query(\"SELECT * FROM age_group;\", crm_conn)\n",
    "crm_country           = pd.read_sql_query(\"SELECT * FROM country;\", crm_conn)\n",
    "retailer              = pd.read_sql_query(\"SELECT * FROM retailer;\", crm_conn)\n",
    "retailer_contact      = pd.read_sql_query(\"SELECT * FROM retailer_contact;\", crm_conn)\n",
    "retailer_headquarters = pd.read_sql_query(\"SELECT * FROM retailer_headquarters;\", crm_conn)\n",
    "retailer_segment      = pd.read_sql_query(\"SELECT * FROM retailer_segment;\", crm_conn)\n",
    "crm_retailer_site     = pd.read_sql_query(\"SELECT * FROM retailer_site;\", crm_conn)\n",
    "retailer_type         = pd.read_sql_query(\"SELECT * FROM retailer_type;\", crm_conn)\n",
    "sales_demographic     = pd.read_sql_query(\"SELECT * FROM sales_demographic;\", crm_conn)\n",
    "sales_territory       = pd.read_sql_query(\"SELECT * FROM sales_territory;\", crm_conn)\n",
    "print(\"Imported crm tables\")\n",
    "\n",
    "inventory_level = pd.read_csv(\"GO_SALES_INVENTORY_LEVELSData.csv\")\n",
    "print(\"Imported inventory\")\n",
    "\n",
    "sales_forecast = pd.read_csv(\"GO_SALES_PRODUCT_FORECASTData.csv\")\n",
    "print(\"Imported sales_product_forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Nadat we de data eruit hebben gehaald, gaan de data transformeren, zodat we ze in de database kunnen stoppen. Eerst doen maken we een merge functie van de data die we hebben geÃ«xtraheerd en de data die we al hebben in de database. Daarna gaan we de data transformeren zodat we ze in de database kunnen stoppen. Ik maak wat functies die ik heb gevonden op GitHub waarmee we makkelijk de data kunnen mergen. Hierin kan ik dat makkelijk uitvoeren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flexible method to merge two tables\n",
    "- NaN values of one dataframe can be filled by the other dataframe\n",
    "- Uses all available columns\n",
    "- Errors when a row of the two dataframes doesn't match (df1 has 'A' and df2 has 'B' in row)\n",
    "\"\"\"\n",
    "def merge_tables(df1, df2, index_col):\n",
    "    # Zorg ervoor dat het index_col een kolom is in beide dataframes\n",
    "    if index_col not in df1.columns or index_col not in df2.columns:\n",
    "        raise KeyError(f\"{index_col} must be a column in both DataFrames.\")\n",
    "    \n",
    "    df1 = df1.set_index(index_col)\n",
    "    df2 = df2.set_index(index_col)\n",
    "\n",
    "    # Identificeer de kolommen die in beide dataframes voorkomen\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "    exclusive_df1 = df1.columns.difference(df2.columns)\n",
    "    exclusive_df2 = df2.columns.difference(df1.columns)\n",
    "\n",
    "    # Concatenate exclusive columns from each DataFrame onto the other\n",
    "    df1_combined = pd.concat([df1, df2[exclusive_df2]], axis=1, sort=False)\n",
    "    df2_combined = pd.concat([df2, df1[exclusive_df1]], axis=1, sort=False)\n",
    "\n",
    "    # Los conflicts op in de common columns\n",
    "    for col in common_columns:\n",
    "        # Zet de kolommen van de dataframes naast elkaar\n",
    "        series1, series2 = df1_combined[col].align(df2_combined[col])\n",
    "\n",
    "        # Check voor conflicts die niet opgelost kunnen worden (waar beide dataframes een waarde hebben)\n",
    "        conflict_mask = (~series1.isnull() & ~series2.isnull() & (series1 != series2))\n",
    "        if conflict_mask.any():\n",
    "            raise ValueError(f\"Merge failed due to conflict in column '{col}'\")\n",
    "\n",
    "        # Use values from df2 where df1 is null (prioritizing df1 values)\n",
    "        df1_combined[col] = series1.combine_first(series2)\n",
    "\n",
    "    return df1_combined\n",
    "\n",
    "# Merge duplicate tables into single table\n",
    "retailer_site = merge_tables(sales_retailer_site, crm_retailer_site, 'RETAILER_SITE_CODE')\n",
    "# Column name mismatch\n",
    "sales_country = sales_country.rename(columns={'COUNTRY': 'COUNTRY_EN'})\n",
    "country = merge_tables(sales_country, crm_country, 'COUNTRY_CODE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON importeren\n",
    "Eerst zorgen we ervoor dat we de rename.json gaan importeren, waar ik de nieuwe namen van de kolommen heb gezet. Met deze kolommen gaan we de data mergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeer de json file\n",
    "with open('rename.json') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "# Geef een lijst van alle waardes in de json file\n",
    "valid_columns = list(json_file.values())\n",
    "\n",
    "# Filter de kolommen van de dataframes, door alleen de kolommen te houden die in de json file staan.\n",
    "def filterColumns(dataframe):\n",
    "    valid_columns_set = set(valid_columns)\n",
    "    actual_columns_set = set(dataframe.columns)\n",
    "    intersection_columns = list(actual_columns_set.intersection(valid_columns_set))\n",
    "\n",
    "    # Gebruik de kolommen die in de json file staan om de dataframes te filteren\n",
    "    return dataframe[intersection_columns]\n",
    "\n",
    "# Filter de kolommen van de dataframes, door alleen de kolommen te houden die niet in de json file staan.\n",
    "def excludeColumns(dataframe, column_names):\n",
    "    return dataframe[dataframe.columns.difference(column_names)]\n",
    "\n",
    "# Check de grootte van de dataframes en print een bericht als de grootte niet overeenkomt met de verwachte grootte\n",
    "def sizeCheck(df, expected_column_count):\n",
    "    actual_column_count = len(df.columns)\n",
    "    if actual_column_count == expected_column_count:\n",
    "        print(f'Table has {actual_column_count} columns')\n",
    "    else:\n",
    "        raise Exception(f'Table has {actual_column_count} columns, expected {expected_column_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns aanpassen\n",
    "Ik neem nu even wat code over van Joran zijn notebook, omdat hij een makkelijke manier heeft gegeven om types aan te geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    'name': 'NVARCHAR(255)',\n",
    "    'image': 'NVARCHAR(255)',\n",
    "    'id': 'NVARCHAR(255)',\n",
    "    'description': 'VARCHAR(MAX)',\n",
    "    'money': 'DECIMAL(19,4)',\n",
    "    'percentage': 'DECIMAL(12,12)',\n",
    "    'date': 'NVARCHAR(255)',\n",
    "    'code': 'NVARCHAR(40)',\n",
    "    'char': 'CHAR(1)',\n",
    "    'number': 'INT',\n",
    "    'phone': 'NVARCHAR(30)',\n",
    "    'address': 'NVARCHAR(80)',\n",
    "    'bool': 'BIT',\n",
    "}\n",
    "\n",
    "def getTypes():\n",
    "    types = {}\n",
    "    for column in json_file.values():\n",
    "        column_type = column.rsplit('_', 1)[1]\n",
    "        types[column_type] = ''\n",
    "    return types\n",
    "\n",
    "def columnType(column_name):\n",
    "    err = ''\n",
    "    try:\n",
    "        return column_types[column_name.rsplit('_', 1)[1]]\n",
    "    except IndexError:\n",
    "        err = \"Column name doesn't contain a type\"\n",
    "    except KeyError:\n",
    "        err = \"Column type not found\"\n",
    "    raise Exception(err)\n",
    "\n",
    "def createTable(dataframe, PK):\n",
    "    # Primaire sleutel waarvan de type-extensie is verwijderd\n",
    "    # Handarbeid is het niet waard!\n",
    "    tablename = PK.rsplit('_', 1)[0]\n",
    "\n",
    "    # Voeg primaire sleutel toe als eerste kolom\n",
    "    columns = f'{PK} {columnType(PK)} NOT NULL PRIMARY KEY'\n",
    "\n",
    "    # Voeg alle andere kolommen toe\n",
    "    for column in dataframe.columns:\n",
    "        if column != PK: # PK is al toegevoegd\n",
    "            columns += f', {column} {columnType(column)}'\n",
    "\n",
    "    # Maak de tabel command aan.\n",
    "    command = f\"CREATE TABLE {tablename} ({columns})\"\n",
    "\n",
    "    # Hier executeren we de create table command die we net hebben aangemaakt en het is een try-except block om te controleren of de tabel al bestaat.\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        cursor.commit()\n",
    "        columns = ', '.join(dataframe.columns)\n",
    "        placeholders = ', '.join(['?' for _ in range(len(dataframe.columns))])\n",
    "        insert_sql = f\"INSERT INTO {tablename} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "        # Elke rij in het DataFrame doorlopen en in de tabel invoegen\n",
    "        for index, row in dataframe.iterrows():\n",
    "            # Hier checken we of de primaire sleutel al bestaat in de tabel\n",
    "            key_exists_sql = f\"SELECT COUNT(*) FROM {tablename} WHERE {PK} = ?\"\n",
    "\n",
    "            # Hier voeren we de SQL-opdracht uit met de waarden uit de huidige rij van het DataFrame\n",
    "            cursor.execute(key_exists_sql, (row[PK],))\n",
    "\n",
    "            # Hier halen we de eerste kolom van de eerste rij op, die het aantal keren dat de primaire sleutel voorkomt\n",
    "            key_exists = cursor.fetchone()[0]\n",
    "\n",
    "            # Als de primaire sleutel niet bestaat, voegen we de rij toe aan de tabel\n",
    "            if key_exists == 0:\n",
    "                # Het uitvoeren van de SQL-opdracht met de waarden uit de huidige rij van het DataFrame\n",
    "                cursor.execute(insert_sql, tuple(row))\n",
    "                cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        if 'There is already an object named' in str(e):\n",
    "            print('Table already exists in database')\n",
    "        else:\n",
    "            raise(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we eindelijk de data transformeren en in de database stoppen. Eerst gaan we producten, staff, satisfaction, course, sales_forcast, retailer_contact, retailer, Orders, returned_season, returned_item en Order_details importeren. Dit zijn de tabellen die we hebben gemaakt in de database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "### Producten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 10 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "product_etl = pd.merge(product, product_type, on=\"PRODUCT_TYPE_CODE\")\n",
    "product_etl = pd.merge(product_etl, product_line, on=\"PRODUCT_LINE_CODE\")\n",
    "\n",
    "# Hernoem\n",
    "product_etl = product_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "product_etl = filterColumns(product_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(product_etl,10)\n",
    "product_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(product_etl, 'PRODUCT_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales_staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 23 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "sales_staff_etl = pd.merge(sales_staff, sales_branch, on='SALES_BRANCH_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, country, on='COUNTRY_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, sales_territory, on='SALES_TERRITORY_CODE')\n",
    "\n",
    "# Hernoem\n",
    "sales_staff_etl = sales_staff_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "sales_staff_etl = filterColumns(sales_staff_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_staff_etl,23)\n",
    "sales_staff_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(sales_staff_etl, 'SALES_STAFF_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfaction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "satisfaction_type_etl = satisfaction_type.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "satisfaction_type_etl = filterColumns(satisfaction_type_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(satisfaction_type_etl,2)\n",
    "satisfaction_type_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(satisfaction_type_etl, 'SATISFACTION_TYPE_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "course_etl = course.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "course_etl = filterColumns(course_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(course_etl,2)\n",
    "course_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(course_etl, 'COURSE_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 3 columns\n",
      "Table already exists in database\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "sales_forecast_etl = sales_forecast.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "sales_forecast_etl = filterColumns(sales_forecast_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_forecast_etl,3)\n",
    "sales_forecast_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(sales_forecast_etl, 'PRODUCT_NUMBER_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retailer_contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 23 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_contact_etl = pd.merge(retailer_contact, retailer_site, on='RETAILER_SITE_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, country, on='COUNTRY_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    \n",
    "# Hernoem \n",
    "retailer_contact_etl = retailer_contact_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "retailer_contact_etl = filterColumns(retailer_contact_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(retailer_contact_etl,23)\n",
    "retailer_contact_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(retailer_contact_etl, 'RETAILER_CONTACT_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 22 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_etl = pd.merge(retailer, retailer_headquarters, on='RETAILER_CODEMR')\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_type, on='RETAILER_TYPE_CODE')\n",
    "\n",
    "# Merge en hernoem de taal kolommen via de country tabel en retailer_segment tabel\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_segment, on='SEGMENT_CODE').rename(columns={'LANGUAGE':'SEGMENT_LANGUAGE_code'})\n",
    "retailer_etl = pd.merge(retailer_etl, country, on='COUNTRY_CODE').rename(columns={'LANGUAGE':'COUNTRY_LANGUAGE_code'})\n",
    "\n",
    "# Sluit kolommen vroegtijdig uit vanwege samenvoegingsnaamconflicten, want duidelijk creert SQL Server deze kolommen.\n",
    "retailer_etl = excludeColumns(retailer_etl, ['TRIAL219','TRIAL222_x','TRIAL222_y','TRIAL222'])\n",
    "\n",
    "# Hernoem\n",
    "retailer_etl = pd.merge(retailer_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    .rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "retailer_etl = filterColumns(retailer_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(retailer_etl,22)\n",
    "\n",
    "# Create Table\n",
    "createTable(retailer_etl, 'RETAILER_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "order_etl = pd.merge(order_header, order_method, on='ORDER_METHOD_CODE').rename(columns=json_file)\n",
    "\n",
    "# Sluit redundante kolommen met externe sleutels uit\n",
    "# RETAILER_SITE_code word afgeleid van RETAILER_CONTACT_id\n",
    "# SALES_BRANCH_code word afgeleid van SALES_STAFF_id\n",
    "order_etl = excludeColumns(order_etl, ['RETAILER_SITE_id', 'SALES_BRANCH_id'])\n",
    "\n",
    "order_etl.reset_index(inplace=True)\n",
    "order_etl.rename(columns={'index': 'SURROGATE_KEY'}, inplace=True)\n",
    "\n",
    "# Filter\n",
    "order_etl = filterColumns(order_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(order_etl,7)\n",
    "order_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(order_etl, 'ORDERS_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "return_reason_etl = return_reason.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "return_reason_etl = filterColumns(return_reason_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(return_reason_etl,2)\n",
    "return_reason_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(return_reason_etl, 'RETURN_REASON_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem \n",
    "returned_item_etl = returned_item.rename(columns=json_file)\n",
    "\n",
    "# Filter \n",
    "returned_item_etl = filterColumns(returned_item_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(returned_item_etl,5)\n",
    "returned_item_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(returned_item_etl, 'RETURNS_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "order_detail_etl = order_details.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "order_detail_etl = filterColumns(order_detail_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(order_detail_etl,7)\n",
    "order_detail_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(order_detail_etl, 'ORDER_DETAIL_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder gaan we de data inladen vanuit de SQL Server database. Met de database verzorgen we ervoor dat we makkelijk de data kunnen inladen in de database. We maken een functie die de data inlaad in de database. Dat deden we all gedeelte lijk boven, maar nu gaan we de data uit de database halen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCT\n",
      "SALES_STAFF\n",
      "SATISFACTION_TYPE\n",
      "COURSE\n",
      "RETAILER_CONTACT\n",
      "RETAILER\n",
      "ORDERS\n",
      "RETURN_REASON\n",
      "RETURNS\n",
      "ORDER_DETAIL\n",
      "[('8', Decimal('75.0000'), 'EN', 'Camping Equipment', 'The TrailChef Double Flame is a camp stove which attaches directly to fuel canisters. Features precise flame control and a built-in windscreen.  Weight is 850 g, fuel is propane.', 'TrailChef Double Flame', '1', 'P08CE1CG1.jpg', '5-3-2013', Decimal('0.410000000000'))]\n"
     ]
    }
   ],
   "source": [
    "tables = cursor.execute(\"SELECT t.name FROM sys.tables t\")\n",
    "tables = tables.fetchall()\n",
    "for table in tables:\n",
    "    table = table[0]\n",
    "    print(table)\n",
    "\n",
    "leef = cursor.execute(\"SELECT * FROM PRODUCT WHERE PRODUCT_id = '8'\")\n",
    "print(leef.fetchall())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warehouse-YJ77YNfe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
