{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR 4.3\n",
    "Van Pjotr en Sennen\n",
    "\n",
    "Hierin gaan wij een paar queries aanvragen aan de database die wij hebben gemaakt gebasseerd op de ETL diagram van de Great_Outdoors.\n",
    "\n",
    "Hieronder zullen we beginnen met de setup van de libraries en het verbinden met de database. We zullen de pyodbc library gebruiken om de verbinding tussen SSMS en Python. Verder gebruiken we ook pandas om data makkelijk te lezen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Cursor object at 0x000001CA2D147EB0>\n"
     ]
    }
   ],
   "source": [
    "DB = {'servername': os.getenv('NAME'),\n",
    "      'database': os.getenv('DATABASE'),\n",
    "      'username': os.getenv('USER'),\n",
    "      'password': os.getenv('PASSWORD')}\n",
    "\n",
    "# Dit is de connectie string voor de SQL Server\n",
    "conn_str = f\"DRIVER=SQL Server;SERVER={DB['servername']};DATABASE={DB['database']};UID={DB['username']};PWD={DB['password']};Trusted_Connection=yes;\"\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "# Hoe checken we of de connectie werkt?\n",
    "print(cursor.execute(\"SELECT @@version;\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om ervoor te zorgen dat we weten dat we data kunnen aanvragen vanaf de server zullen we hier een paar queries executeren. Zodat we kunnen bevestigen dat we alles goed hebben geconfigureerd. Eerst pakken we alle tabelen van uit de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "S\n",
      "S\n",
      "C\n",
      "R\n",
      "R\n",
      "R\n",
      "R\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT t.name FROM sys.tables t\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Voor elke tabel in de database print de naam van de tabel\n",
    "if(tables == []):\n",
    "    print(\"No tables found, the database is empty.\")\n",
    "else:\n",
    "    for table in tables:\n",
    "        table = table[0]\n",
    "        print(table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder checken we of we alle drivers hebben geinstalleerd. Meestal maken we gebruikt van de SQL Server driver en SQLite driver. Maar als je op een nieuwer systeem zit kan je ook gebruik maken van de Microsoft Access driver gebruik maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SQL Server',\n",
       " 'MySQL ODBC 8.0 ANSI Driver',\n",
       " 'MySQL ODBC 8.0 Unicode Driver',\n",
       " 'SQL Server Native Client RDA 11.0',\n",
       " 'ODBC Driver 17 for SQL Server',\n",
       " 'Microsoft Access Driver (*.mdb, *.accdb)',\n",
       " 'Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)',\n",
       " 'Microsoft Access Text Driver (*.txt, *.csv)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daarna wat we willen zijn dus de functies maken die we later in het project gaan gebruiken om de data in te laden in ons project. Zodat we de data uit de bron kunnen transformeren en overzetten naar onze SQL Server database. We maken hiervoor een extract functie maken en een laad functie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "Eerst gaan we de data uit de access database halen en ervoor zorgen dat we ze later goed kunnen transformeren in de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import sales\n",
      "Imported staff\n",
      "Imported crm tables\n",
      "Imported inventory\n",
      "Imported sales_product_forecast\n"
     ]
    }
   ],
   "source": [
    "select_tables = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "\n",
    "# Verbind met sqlite go_sales staff\n",
    "sales_conn = sqlite3.connect(\"go_sales.sqlite\")\n",
    "sales_tables = pd.read_sql_query(select_tables, sales_conn)\n",
    "\n",
    "sales_country       = pd.read_sql_query(\"SELECT * FROM country;\", sales_conn)\n",
    "order_details       = pd.read_sql_query(\"SELECT * FROM order_details;\", sales_conn)\n",
    "order_header        = pd.read_sql_query(\"SELECT * FROM order_header;\", sales_conn)\n",
    "order_method        = pd.read_sql_query(\"SELECT * FROM order_method;\", sales_conn)\n",
    "product             = pd.read_sql_query(\"SELECT * FROM product;\", sales_conn)\n",
    "product_line        = pd.read_sql_query(\"SELECT * FROM product_line;\", sales_conn)\n",
    "product_type        = pd.read_sql_query(\"SELECT * FROM product_type;\", sales_conn)\n",
    "sales_retailer_site = pd.read_sql_query(\"SELECT * FROM retailer_site;\", sales_conn)\n",
    "return_reason       = pd.read_sql_query(\"SELECT * FROM return_reason;\", sales_conn)\n",
    "returned_item       = pd.read_sql_query(\"SELECT * FROM returned_item;\", sales_conn)\n",
    "sales_branch        = pd.read_sql_query(\"SELECT * FROM sales_branch;\", sales_conn)\n",
    "sales_staff         = pd.read_sql_query(\"SELECT * FROM sales_staff;\", sales_conn)\n",
    "SALES_TARGETData    = pd.read_sql_query(\"SELECT * FROM SALES_TARGETData;\", sales_conn)\n",
    "sqlite_sequence     = pd.read_sql_query(\"SELECT * FROM sqlite_sequence;\", sales_conn)\n",
    "print(\"Import sales\")\n",
    "\n",
    "staff_conn = sqlite3.connect(\"go_staff.sqlite\")\n",
    "staff_tables = pd.read_sql_query(select_tables, staff_conn)\n",
    "course            = pd.read_sql_query(\"SELECT * FROM course;\", staff_conn)\n",
    "sales_branch      = pd.read_sql_query(\"SELECT * FROM sales_branch;\", staff_conn)\n",
    "sales_staff       = pd.read_sql_query(\"SELECT * FROM sales_staff;\", staff_conn)\n",
    "satisfaction      = pd.read_sql_query(\"SELECT * FROM satisfaction;\", staff_conn)\n",
    "satisfaction_type = pd.read_sql_query(\"SELECT * FROM satisfaction_type;\", staff_conn)\n",
    "training          = pd.read_sql_query(\"SELECT * FROM training;\", staff_conn)\n",
    "print(\"Imported staff\")\n",
    "\n",
    "crm_conn = sqlite3.connect(\"go_crm.sqlite\")\n",
    "crm_tables = pd.read_sql_query(select_tables, crm_conn)\n",
    "                           \n",
    "age_group             = pd.read_sql_query(\"SELECT * FROM age_group;\", crm_conn)\n",
    "crm_country           = pd.read_sql_query(\"SELECT * FROM country;\", crm_conn)\n",
    "retailer              = pd.read_sql_query(\"SELECT * FROM retailer;\", crm_conn)\n",
    "retailer_contact      = pd.read_sql_query(\"SELECT * FROM retailer_contact;\", crm_conn)\n",
    "retailer_headquarters = pd.read_sql_query(\"SELECT * FROM retailer_headquarters;\", crm_conn)\n",
    "retailer_segment      = pd.read_sql_query(\"SELECT * FROM retailer_segment;\", crm_conn)\n",
    "crm_retailer_site     = pd.read_sql_query(\"SELECT * FROM retailer_site;\", crm_conn)\n",
    "retailer_type         = pd.read_sql_query(\"SELECT * FROM retailer_type;\", crm_conn)\n",
    "sales_demographic     = pd.read_sql_query(\"SELECT * FROM sales_demographic;\", crm_conn)\n",
    "sales_territory       = pd.read_sql_query(\"SELECT * FROM sales_territory;\", crm_conn)\n",
    "print(\"Imported crm tables\")\n",
    "\n",
    "inventory_level = pd.read_csv(\"GO_SALES_INVENTORY_LEVELSData.csv\")\n",
    "print(\"Imported inventory\")\n",
    "\n",
    "sales_forecast = pd.read_csv(\"GO_SALES_PRODUCT_FORECASTData.csv\")\n",
    "print(\"Imported sales_product_forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Nadat we de data eruit hebben gehaald, gaan de data transformeren, zodat we ze in de database kunnen stoppen. Eerst doen maken we een merge functie van de data die we hebben geÃ«xtraheerd en de data die we al hebben in de database. Daarna gaan we de data transformeren zodat we ze in de database kunnen stoppen. Ik maak wat functies die ik heb gevonden op GitHub waarmee we makkelijk de data kunnen mergen. Hierin kan ik dat makkelijk uitvoeren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flexible method to merge two tables\n",
    "- NaN values of one dataframe can be filled by the other dataframe\n",
    "- Uses all available columns\n",
    "- Errors when a row of the two dataframes doesn't match (df1 has 'A' and df2 has 'B' in row)\n",
    "\"\"\"\n",
    "def merge_tables(df1, df2, index_col):\n",
    "    # Zorg ervoor dat het index_col een kolom is in beide dataframes\n",
    "    if index_col not in df1.columns or index_col not in df2.columns:\n",
    "        raise KeyError(f\"{index_col} must be a column in both DataFrames.\")\n",
    "    \n",
    "    df1 = df1.set_index(index_col)\n",
    "    df2 = df2.set_index(index_col)\n",
    "\n",
    "    # Identificeer de kolommen die in beide dataframes voorkomen\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "    exclusive_df1 = df1.columns.difference(df2.columns)\n",
    "    exclusive_df2 = df2.columns.difference(df1.columns)\n",
    "\n",
    "    # Concatenate exclusive columns from each DataFrame onto the other\n",
    "    df1_combined = pd.concat([df1, df2[exclusive_df2]], axis=1, sort=False)\n",
    "    df2_combined = pd.concat([df2, df1[exclusive_df1]], axis=1, sort=False)\n",
    "\n",
    "    # Los conflicts op in de common columns\n",
    "    for col in common_columns:\n",
    "        # Zet de kolommen van de dataframes naast elkaar\n",
    "        series1, series2 = df1_combined[col].align(df2_combined[col])\n",
    "\n",
    "        # Check voor conflicts die niet opgelost kunnen worden (waar beide dataframes een waarde hebben)\n",
    "        conflict_mask = (~series1.isnull() & ~series2.isnull() & (series1 != series2))\n",
    "        if conflict_mask.any():\n",
    "            raise ValueError(f\"Merge failed due to conflict in column '{col}'\")\n",
    "\n",
    "        # Use values from df2 where df1 is null (prioritizing df1 values)\n",
    "        df1_combined[col] = series1.combine_first(series2)\n",
    "\n",
    "    return df1_combined\n",
    "\n",
    "# Merge duplicate tables into single table\n",
    "retailer_site = merge_tables(sales_retailer_site, crm_retailer_site, 'RETAILER_SITE_CODE')\n",
    "# Column name mismatch\n",
    "sales_country = sales_country.rename(columns={'COUNTRY': 'COUNTRY_EN'})\n",
    "country = merge_tables(sales_country, crm_country, 'COUNTRY_CODE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON importeren\n",
    "Eerst zorgen we ervoor dat we de rename.json gaan importeren, waar ik de nieuwe namen van de kolommen heb gezet. Met deze kolommen gaan we de data mergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeer de json file\n",
    "with open('rename.json') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "# Geef een lijst van alle waardes in de json file\n",
    "valid_columns = list(json_file.values())\n",
    "\n",
    "# Filter de kolommen van de dataframes, door alleen de kolommen te houden die in de json file staan.\n",
    "def filterColumns(dataframe):\n",
    "    valid_columns_set = set(valid_columns)\n",
    "    actual_columns_set = set(dataframe.columns)\n",
    "    intersection_columns = list(actual_columns_set.intersection(valid_columns_set))\n",
    "\n",
    "    # Gebruik de kolommen die in de json file staan om de dataframes te filteren\n",
    "    return dataframe[intersection_columns]\n",
    "\n",
    "# Filter de kolommen van de dataframes, door alleen de kolommen te houden die niet in de json file staan.\n",
    "def excludeColumns(dataframe, column_names):\n",
    "    return dataframe[dataframe.columns.difference(column_names)]\n",
    "\n",
    "# Check de grootte van de dataframes en print een bericht als de grootte niet overeenkomt met de verwachte grootte\n",
    "def sizeCheck(df, expected_column_count):\n",
    "    actual_column_count = len(df.columns)\n",
    "    if actual_column_count == expected_column_count:\n",
    "        print(f'Table has {actual_column_count} columns')\n",
    "    else:\n",
    "        raise Exception(f'Table has {actual_column_count} columns, expected {expected_column_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns aanpassen\n",
    "Ik neem nu even wat code over van Joran zijn notebook, omdat hij een makkelijke manier heeft gegeven om types aan te geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    'name': 'NVARCHAR(80)',\n",
    "    'image': 'NVARCHAR(60)',\n",
    "    'id': 'INT',\n",
    "    'description': 'NTEXT',\n",
    "    'money': 'DECIMAL(19,4)',\n",
    "    'percentage': 'DECIMAL(12,12)',\n",
    "    'date': 'NVARCHAR(30)',\n",
    "    'code': 'NVARCHAR(40)',\n",
    "    'char': 'CHAR(1)',\n",
    "    'number': 'INT',\n",
    "    'phone': 'NVARCHAR(30)',\n",
    "    'address': 'NVARCHAR(80)',\n",
    "    'bool': 'BIT',\n",
    "}\n",
    "\n",
    "def getTypes():\n",
    "    types = {}\n",
    "    for column in json_file.values():\n",
    "        column_type = column.rsplit('_', 1)[1]\n",
    "        types[column_type] = ''\n",
    "    return types\n",
    "\n",
    "def columnType(column_name):\n",
    "    err = ''\n",
    "    try:\n",
    "        return column_types[column_name.rsplit('_', 1)[1]]\n",
    "    except IndexError:\n",
    "        err = \"Column name doesn't contain a type\"\n",
    "    except KeyError:\n",
    "        err = \"Column type not found\"\n",
    "    raise Exception(err)\n",
    "\n",
    "def createTable(dataframe, PK):\n",
    "    # Primary key with the type extension removed\n",
    "    # Manual labor isn't worth it!\n",
    "    tablename = PK.rsplit('_', 1)[0]\n",
    "\n",
    "    # Add Primary Key as first column\n",
    "    columns = f'{PK} {columnType(PK)} NOT NULL PRIMARY KEY'\n",
    "\n",
    "    # Add all the other columns\n",
    "    for column in dataframe.columns:\n",
    "        if column != PK: # PK is already added\n",
    "            columns += f', {column} {columnType(column)}'\n",
    "\n",
    "    # Create the command\n",
    "    command = f\"CREATE TABLE {tablename} ({columns})\"\n",
    "\n",
    "    print(command)\n",
    "\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        if 'There is already an object named' in str(e):\n",
    "            print('Table already exists in database')\n",
    "        else:\n",
    "            raise(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we eindelijk de data transformeren en in de database stoppen. Eerst gaan we producten, staff, satisfaction, course, sales_forcast, retailer_contact, retailer, Orders, returned_season, returned_item en Order_details importeren. Dit zijn de tabellen die we hebben gemaakt in de database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "### Producten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 10 columns\n",
      "CREATE TABLE PRODUCT (PRODUCT_id INT NOT NULL PRIMARY KEY, PRODUCT_image NVARCHAR(60), PRODUCT_MARGIN_percentage DECIMAL(12,12), PRODUCT_name NVARCHAR(80), LANGUAGE_name NVARCHAR(80), PRODUCT_LINE_name NVARCHAR(80), PRODUCT_PRODUCTION_COST_money DECIMAL(19,4), PRODUCT_INTRODUCTION_DATE_date NVARCHAR(30), PRODUCT_description NTEXT, PRODUCT_LINE_id INT)\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "product_etl = pd.merge(product, product_type, on=\"PRODUCT_TYPE_CODE\")\n",
    "product_etl = pd.merge(product_etl, product_line, on=\"PRODUCT_LINE_CODE\")\n",
    "\n",
    "# Hernoem\n",
    "product_etl = product_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "product_etl = filterColumns(product_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(product_etl,10)\n",
    "product_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(product_etl, 'PRODUCT_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales_staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 23 columns\n",
      "CREATE TABLE SALES_STAFF (SALES_STAFF_id INT NOT NULL PRIMARY KEY, FLAG_image NVARCHAR(60), POSITION_name NVARCHAR(80), ADDRESS1_address NVARCHAR(80), LAST_NAME_name NVARCHAR(80), REGION_name NVARCHAR(80), MANAGER_id INT, COUNTRY_name NVARCHAR(80), SALES_TERRITORY_id INT, EMAIL_address NVARCHAR(80), FIRST_NAME_name NVARCHAR(80), POSTAL_ZONE_code NVARCHAR(40), ADDRESS2_address NVARCHAR(80), WORK_PHONE_phone NVARCHAR(30), CURRENCY_name NVARCHAR(80), SALES_BRANCH_id INT, EXTENSION_number INT, LANGUAGE_name NVARCHAR(80), TERRITORY_name NVARCHAR(80), CITY_name NVARCHAR(80), DATE_HIRED_date NVARCHAR(30), COUNTRY_id INT, FAX_phone NVARCHAR(30))\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "sales_staff_etl = pd.merge(sales_staff, sales_branch, on='SALES_BRANCH_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, country, on='COUNTRY_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, sales_territory, on='SALES_TERRITORY_CODE')\n",
    "\n",
    "# Hernoem\n",
    "sales_staff_etl = sales_staff_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "sales_staff_etl = filterColumns(sales_staff_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_staff_etl,23)\n",
    "sales_staff_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(sales_staff_etl, 'SALES_STAFF_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfaction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n",
      "CREATE TABLE SATISFACTION_TYPE (SATISFACTION_TYPE_id INT NOT NULL PRIMARY KEY, SATISFACTION_TYPE_description NTEXT)\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "satisfaction_type_etl = satisfaction_type.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "satisfaction_type_etl = filterColumns(satisfaction_type_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(satisfaction_type_etl,2)\n",
    "satisfaction_type_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(satisfaction_type_etl, 'SATISFACTION_TYPE_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n",
      "CREATE TABLE COURSE (COURSE_id INT NOT NULL PRIMARY KEY, COURSE_description NTEXT)\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "course_etl = course.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "course_etl = filterColumns(course_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(course_etl,2)\n",
    "course_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(course_etl, 'COURSE_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 4 columns\n",
      "CREATE TABLE PRODUCT (PRODUCT_id INT NOT NULL PRIMARY KEY, MONTH_number INT, EXPECTED_VOLUME_number INT, YEAR_number INT)\n",
      "Table already exists in database\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "sales_forecast_etl = sales_forecast.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "sales_forecast_etl = filterColumns(sales_forecast_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_forecast_etl,4)\n",
    "sales_forecast_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(sales_forecast_etl, 'PRODUCT_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retailer_contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 23 columns\n",
      "CREATE TABLE RETAILER_CONTACT (RETAILER_CONTACT_id INT NOT NULL PRIMARY KEY, FLAG_image NVARCHAR(60), ADDRESS1_address NVARCHAR(80), JOB_POSITION_name NVARCHAR(80), LAST_NAME_name NVARCHAR(80), REGION_name NVARCHAR(80), GENDER_char CHAR(1), COUNTRY_name NVARCHAR(80), SALES_TERRITORY_id INT, EMAIL_address NVARCHAR(80), RETAILER_id INT, FIRST_NAME_name NVARCHAR(80), POSTAL_ZONE_code NVARCHAR(40), ADDRESS2_address NVARCHAR(80), CURRENCY_name NVARCHAR(80), RETAILER_SITE_id INT, EXTENSION_number INT, LANGUAGE_name NVARCHAR(80), TERRITORY_name NVARCHAR(80), CITY_name NVARCHAR(80), ACTIVE_INDICATOR_bool BIT, COUNTRY_id INT, FAX_phone NVARCHAR(30))\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_contact_etl = pd.merge(retailer_contact, retailer_site, on='RETAILER_SITE_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, country, on='COUNTRY_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    \n",
    "# Hernoem \n",
    "retailer_contact_etl = retailer_contact_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "retailer_contact_etl = filterColumns(retailer_contact_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(retailer_contact_etl,23)\n",
    "retailer_contact_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(retailer_contact_etl, 'RETAILER_CONTACT_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 22 columns\n",
      "CREATE TABLE RETAILER (RETAILER_id INT NOT NULL PRIMARY KEY, FLAG_image NVARCHAR(60), RETAILER_MR_id INT, ADDRESS1_address NVARCHAR(80), SEGMENT_LANGUAGE_id INT, PHONE_phone NVARCHAR(30), COMPANY_name NVARCHAR(80), REGION_name NVARCHAR(80), COUNTRY_name NVARCHAR(80), SALES_TERRITORY_id INT, RETAILER_TYPE_id INT, RETAILER_TYPE_name NVARCHAR(80), POSTAL_ZONE_code NVARCHAR(40), ADDRESS2_address NVARCHAR(80), CURRENCY_name NVARCHAR(80), COUNTRY_LANGUAGE_id INT, RETAILER_name NVARCHAR(80), TERRITORY_name NVARCHAR(80), SEGMENT_code NVARCHAR(40), CITY_name NVARCHAR(80), COUNTRY_id INT, FAX_phone NVARCHAR(30))\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_etl = pd.merge(retailer, retailer_headquarters, on='RETAILER_CODEMR')\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_type, on='RETAILER_TYPE_CODE')\n",
    "\n",
    "# Merge en hernoem de taal kolommen via de country tabel en retailer_segment tabel\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_segment, on='SEGMENT_CODE').rename(columns={'LANGUAGE':'SEGMENT_LANGUAGE_code'})\n",
    "retailer_etl = pd.merge(retailer_etl, country, on='COUNTRY_CODE').rename(columns={'LANGUAGE':'COUNTRY_LANGUAGE_code'})\n",
    "\n",
    "# Sluit kolommen vroegtijdig uit vanwege samenvoegingsnaamconflicten, want duidelijk creert SQL Server deze kolommen.\n",
    "retailer_etl = excludeColumns(retailer_etl, ['TRIAL219','TRIAL222_x','TRIAL222_y','TRIAL222'])\n",
    "\n",
    "# Hernoem\n",
    "retailer_etl = pd.merge(retailer_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    .rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "retailer_etl = filterColumns(retailer_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(retailer_etl,22)\n",
    "\n",
    "# Create Table\n",
    "createTable(retailer_etl, 'RETAILER_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n",
      "CREATE TABLE ORDERS (ORDERS_id INT NOT NULL PRIMARY KEY, RETAILER_CONTACT_id INT, ORDER_DATE_date NVARCHAR(30), ORDER_METHOD_id INT, ORDER_id INT, ORDER_METHOD_name NVARCHAR(80), RETAILER_name NVARCHAR(80), SALES_STAFF_id INT)\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "order_etl = pd.merge(order_header, order_method, on='ORDER_METHOD_CODE').rename(columns=json_file)\n",
    "\n",
    "# Sluit redundante kolommen met externe sleutels uit\n",
    "# RETAILER_SITE_code word afgeleid van RETAILER_CONTACT_id\n",
    "# SALES_BRANCH_code word afgeleid van SALES_STAFF_id\n",
    "order_etl = excludeColumns(order_etl, ['RETAILER_SITE_id', 'SALES_BRANCH_id'])\n",
    "\n",
    "order_etl.reset_index(inplace=True)\n",
    "order_etl.rename(columns={'index': 'SURROGATE_KEY'}, inplace=True)\n",
    "\n",
    "# Filter\n",
    "order_etl = filterColumns(order_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(order_etl,7)\n",
    "order_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(order_etl, 'ORDERS_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n",
      "CREATE TABLE RETURN_REASON (RETURN_REASON_id INT NOT NULL PRIMARY KEY, RETURN_REASON_description NTEXT)\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "return_reason_etl = return_reason.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "return_reason_etl = filterColumns(return_reason_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(return_reason_etl,2)\n",
    "return_reason_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(return_reason_etl, 'RETURN_REASON_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 5 columns\n",
      "CREATE TABLE RETURNS (RETURNS_id INT NOT NULL PRIMARY KEY, RETURN_REASON_id INT, ORDER_DETAIL_id INT, RETURN_DATE_date NVARCHAR(30), RETURN_QUANTITY_number INT)\n"
     ]
    }
   ],
   "source": [
    "# Hernoem \n",
    "returned_item_etl = returned_item.rename(columns=json_file)\n",
    "\n",
    "# Filter \n",
    "returned_item_etl = filterColumns(returned_item_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(returned_item_etl,5)\n",
    "returned_item_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(returned_item_etl, 'RETURNS_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n",
      "CREATE TABLE ORDER_DETAIL (ORDER_DETAIL_id INT NOT NULL PRIMARY KEY, UNIT_COST_money DECIMAL(19,4), UNIT_PRICE_money DECIMAL(19,4), UNIT_SALE_PRICE_money DECIMAL(19,4), ORDER_TABLE_id INT, QUANTITY_number INT, PRODUCT_id INT)\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "order_detail_etl = order_details.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "order_detail_etl = filterColumns(order_detail_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(order_detail_etl,7)\n",
    "order_detail_etl\n",
    "\n",
    "# Create Table\n",
    "createTable(order_detail_etl, 'ORDER_DETAIL_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder gaan we de data inladen vanuit de SQL Server database. Met de database verzorgen we ervoor dat we makkelijk de data kunnen inladen in de database. We maken een functie die de data inlaad in de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warehouse-YJ77YNfe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
