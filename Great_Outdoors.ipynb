{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR 4.3, 5.2 en 5.3\n",
    "Van Pjotr en Sennen\n",
    "\n",
    "Hierin gaan wij een paar queries aanvragen aan de database die wij hebben gemaakt gebasseerd op de ETL diagram van de Great_Outdoors.\n",
    "\n",
    "Hieronder zullen we beginnen met de setup van de libraries en het verbinden met de database. We zullen de pyodbc library gebruiken om de verbinding tussen SSMS en Python. Verder gebruiken we ook pandas om data makkelijk te lezen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os\n",
    "import sqlite3\n",
    "from dotenv import load_dotenv\n",
    "import sqlalchemy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Cursor object at 0x00000182F1EF0EB0>\n"
     ]
    }
   ],
   "source": [
    "DB = {'servername': os.getenv('NAME'),\n",
    "      'database': os.getenv('DATABASE'),\n",
    "      'username': os.getenv('USER'),\n",
    "      'password': os.getenv('PASSWORD')}\n",
    "\n",
    "# Increase the connection timeout value to 30 seconds\n",
    "conn_str = f\"DRIVER=SQL Server;SERVER={DB['servername']};DATABASE={DB['database']};UID={DB['username']};PWD={DB['password']};Trusted_Connection=yes;Connection\"\n",
    "\n",
    "conn = pyodbc.connect(conn_str, timeout=120)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "engine = sqlalchemy.create_engine(f\"mssql+pyodbc:///?odbc_connect={conn_str}\")\n",
    "\n",
    "# Hoe checken we of de connectie werkt?\n",
    "print(cursor.execute(\"SELECT @@version;\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om ervoor te zorgen dat we weten dat we data kunnen aanvragen vanaf de server zullen we hier een paar queries executeren. Zodat we kunnen bevestigen dat we alles goed hebben geconfigureerd. Eerst pakken we alle tabelen van uit de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "Sales_Staff\n",
      "Satisfaction_Type\n",
      "Course\n",
      "Sales_Forecast\n",
      "Retailer_Contact\n",
      "Retailer\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"SELECT t.name FROM sys.tables t\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Voor elke tabel in de database print de naam van de tabel\n",
    "if(tables == []):\n",
    "    print(\"No tables found, the database is empty.\")\n",
    "else:\n",
    "    for table in tables:\n",
    "        table = table[0]\n",
    "        print(table)\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        cursor.execute(f\"DROP TABLE {table_name}\")\n",
    "    try:\n",
    "        cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        print(\"Error:\", e)\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder checken we of we alle drivers hebben geinstalleerd. Meestal maken we gebruikt van de SQL Server driver en SQLite driver. Maar als je op een nieuwer systeem zit kan je ook gebruik maken van de Microsoft Access driver gebruik maken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SQL Server',\n",
       " 'MySQL ODBC 8.0 ANSI Driver',\n",
       " 'MySQL ODBC 8.0 Unicode Driver',\n",
       " 'SQL Server Native Client RDA 11.0',\n",
       " 'ODBC Driver 17 for SQL Server',\n",
       " 'Microsoft Access Driver (*.mdb, *.accdb)',\n",
       " 'Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)',\n",
       " 'Microsoft Access Text Driver (*.txt, *.csv)']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daarna wat we willen zijn dus de functies maken die we later in het project gaan gebruiken om de data in te laden in ons project. Zodat we de data uit de bron kunnen transformeren en overzetten naar onze SQL Server database. We maken hiervoor een extract functie maken en een laad functie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract\n",
    "Eerst gaan we de data uit de access database halen en ervoor zorgen dat we ze later goed kunnen transformeren in de database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import sales\n",
      "Imported staff\n",
      "Imported crm tables\n",
      "Imported inventory\n",
      "Imported sales_product_forecast\n"
     ]
    }
   ],
   "source": [
    "select_tables = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "\n",
    "# Verbind met sqlite go_sales staff\n",
    "sales_conn = sqlite3.connect(\"go_sales.sqlite\")\n",
    "sales_tables = pd.read_sql_query(select_tables, sales_conn)\n",
    "\n",
    "sales_country       = pd.read_sql_query(\"SELECT * FROM country;\", sales_conn)\n",
    "order_details       = pd.read_sql_query(\"SELECT * FROM order_details;\", sales_conn)\n",
    "order_header        = pd.read_sql_query(\"SELECT * FROM order_header;\", sales_conn)\n",
    "order_method        = pd.read_sql_query(\"SELECT * FROM order_method;\", sales_conn)\n",
    "product             = pd.read_sql_query(\"SELECT * FROM product;\", sales_conn)\n",
    "product_line        = pd.read_sql_query(\"SELECT * FROM product_line;\", sales_conn)\n",
    "product_type        = pd.read_sql_query(\"SELECT * FROM product_type;\", sales_conn)\n",
    "sales_retailer_site = pd.read_sql_query(\"SELECT * FROM retailer_site;\", sales_conn)\n",
    "return_reason       = pd.read_sql_query(\"SELECT * FROM return_reason;\", sales_conn)\n",
    "returned_item       = pd.read_sql_query(\"SELECT * FROM returned_item;\", sales_conn)\n",
    "sales_branch        = pd.read_sql_query(\"SELECT * FROM sales_branch;\", sales_conn)\n",
    "sales_staff         = pd.read_sql_query(\"SELECT * FROM sales_staff;\", sales_conn)\n",
    "SALES_TARGETData    = pd.read_sql_query(\"SELECT * FROM SALES_TARGETData;\", sales_conn)\n",
    "sqlite_sequence     = pd.read_sql_query(\"SELECT * FROM sqlite_sequence;\", sales_conn)\n",
    "print(\"Import sales\")\n",
    "\n",
    "staff_conn = sqlite3.connect(\"go_staff.sqlite\")\n",
    "staff_tables = pd.read_sql_query(select_tables, staff_conn)\n",
    "course            = pd.read_sql_query(\"SELECT * FROM course;\", staff_conn)\n",
    "sales_branch      = pd.read_sql_query(\"SELECT * FROM sales_branch;\", staff_conn)\n",
    "sales_staff       = pd.read_sql_query(\"SELECT * FROM sales_staff;\", staff_conn)\n",
    "satisfaction      = pd.read_sql_query(\"SELECT * FROM satisfaction;\", staff_conn)\n",
    "satisfaction_type = pd.read_sql_query(\"SELECT * FROM satisfaction_type;\", staff_conn)\n",
    "training          = pd.read_sql_query(\"SELECT * FROM training;\", staff_conn)\n",
    "print(\"Imported staff\")\n",
    "\n",
    "crm_conn = sqlite3.connect(\"go_crm.sqlite\")\n",
    "crm_tables = pd.read_sql_query(select_tables, crm_conn)\n",
    "                           \n",
    "age_group             = pd.read_sql_query(\"SELECT * FROM age_group;\", crm_conn)\n",
    "crm_country           = pd.read_sql_query(\"SELECT * FROM country;\", crm_conn)\n",
    "retailer              = pd.read_sql_query(\"SELECT * FROM retailer;\", crm_conn)\n",
    "retailer_contact      = pd.read_sql_query(\"SELECT * FROM retailer_contact;\", crm_conn)\n",
    "retailer_headquarters = pd.read_sql_query(\"SELECT * FROM retailer_headquarters;\", crm_conn)\n",
    "retailer_segment      = pd.read_sql_query(\"SELECT * FROM retailer_segment;\", crm_conn)\n",
    "crm_retailer_site     = pd.read_sql_query(\"SELECT * FROM retailer_site;\", crm_conn)\n",
    "retailer_type         = pd.read_sql_query(\"SELECT * FROM retailer_type;\", crm_conn)\n",
    "sales_demographic     = pd.read_sql_query(\"SELECT * FROM sales_demographic;\", crm_conn)\n",
    "sales_territory       = pd.read_sql_query(\"SELECT * FROM sales_territory;\", crm_conn)\n",
    "print(\"Imported crm tables\")\n",
    "\n",
    "inventory_level = pd.read_csv(\"GO_SALES_INVENTORY_LEVELSData.csv\")\n",
    "print(\"Imported inventory\")\n",
    "\n",
    "sales_forecast = pd.read_csv(\"GO_SALES_PRODUCT_FORECASTData.csv\")\n",
    "print(\"Imported sales_product_forecast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Nadat we de data eruit hebben gehaald, gaan de data transformeren, zodat we ze in de database kunnen stoppen. Eerst doen maken we een merge functie van de data die we hebben geÃ«xtraheerd en de data die we al hebben in de database. Daarna gaan we de data transformeren zodat we ze in de database kunnen stoppen. Ik maak wat functies die ik heb gevonden op GitHub waarmee we makkelijk de data kunnen mergen. Hierin kan ik dat makkelijk uitvoeren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Functie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flexible method to merge two tables\n",
    "- NaN values of one dataframe can be filled by the other dataframe\n",
    "- Uses all available columns\n",
    "- Errors when a row of the two dataframes doesn't match (df1 has 'A' and df2 has 'B' in row)\n",
    "\"\"\"\n",
    "def merge_tables(df1, df2, index_col):\n",
    "    # Zorg ervoor dat het index_col een kolom is in beide dataframes\n",
    "    if index_col not in df1.columns or index_col not in df2.columns:\n",
    "        raise KeyError(f\"{index_col} must be a column in both DataFrames.\")\n",
    "    \n",
    "    df1 = df1.set_index(index_col)\n",
    "    df2 = df2.set_index(index_col)\n",
    "\n",
    "    # Identificeer de kolommen die in beide dataframes voorkomen\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "    exclusive_df1 = df1.columns.difference(df2.columns)\n",
    "    exclusive_df2 = df2.columns.difference(df1.columns)\n",
    "\n",
    "    # Concatenate exclusive columns from each DataFrame onto the other\n",
    "    df1_combined = pd.concat([df1, df2[exclusive_df2]], axis=1, sort=False)\n",
    "    df2_combined = pd.concat([df2, df1[exclusive_df1]], axis=1, sort=False)\n",
    "\n",
    "    # Los conflicts op in de common columns\n",
    "    for col in common_columns:\n",
    "        # Zet de kolommen van de dataframes naast elkaar\n",
    "        series1, series2 = df1_combined[col].align(df2_combined[col])\n",
    "\n",
    "        # Check voor conflicts die niet opgelost kunnen worden (waar beide dataframes een waarde hebben)\n",
    "        conflict_mask = (~series1.isnull() & ~series2.isnull() & (series1 != series2))\n",
    "        if conflict_mask.any():\n",
    "            raise ValueError(f\"Merge failed due to conflict in column '{col}'\")\n",
    "\n",
    "        # Use values from df2 where df1 is null (prioritizing df1 values)\n",
    "        df1_combined[col] = series1.combine_first(series2)\n",
    "\n",
    "    return df1_combined\n",
    "\n",
    "# Merge duplicate tables into single table\n",
    "retailer_site = merge_tables(sales_retailer_site, crm_retailer_site, 'RETAILER_SITE_CODE')\n",
    "# Column name mismatch\n",
    "sales_country = sales_country.rename(columns={'COUNTRY': 'COUNTRY_EN'})\n",
    "country = merge_tables(sales_country, crm_country, 'COUNTRY_CODE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON importeren\n",
    "Eerst zorgen we ervoor dat we de rename.json gaan importeren, waar ik de nieuwe namen van de kolommen heb gezet. Met deze kolommen gaan we de data mergen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeer de json file\n",
    "with open('rename.json') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "# Geef een lijst van alle waardes in de json file\n",
    "valid_columns = list(json_file.values())\n",
    "\n",
    "# Filter de kolommen van de dataframes, door alleen de kolommen te houden die in de json file staan.\n",
    "def filterColumns(dataframe):\n",
    "    valid_columns_set = set(valid_columns)\n",
    "    actual_columns_set = set(dataframe.columns)\n",
    "    intersection_columns = list(actual_columns_set.intersection(valid_columns_set))\n",
    "\n",
    "    # Gebruik de kolommen die in de json file staan om de dataframes te filteren\n",
    "    return dataframe[intersection_columns]\n",
    "\n",
    "# Filter de kolommen van de dataframes, door alleen de kolommen te houden die niet in de json file staan.\n",
    "def excludeColumns(dataframe, column_names):\n",
    "    return dataframe[dataframe.columns.difference(column_names)]\n",
    "\n",
    "# Check de grootte van de dataframes en print een bericht als de grootte niet overeenkomt met de verwachte grootte\n",
    "def sizeCheck(df, expected_column_count):\n",
    "    actual_column_count = len(df.columns)\n",
    "    if actual_column_count == expected_column_count:\n",
    "        print(f'Table has {actual_column_count} columns')\n",
    "    else:\n",
    "        raise Exception(f'Table has {actual_column_count} columns, expected {expected_column_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns aanpassen\n",
    "Ik neem nu even wat code over van Joran zijn notebook, omdat hij een makkelijke manier heeft gegeven om types aan te geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the last slice of a string\n",
    "\"\"\"\n",
    "def getTypes():\n",
    "    types = {}\n",
    "    for column in json_file.values():\n",
    "        column_type = column.rsplit('_', 1)[1]\n",
    "        types[column_type] = ''\n",
    "    return types\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Uses the column name to derive a SQL Server compatible type\n",
    "- The type is derived from the column name (COLUMN_NAME_type)\n",
    "- Column names without a type are invalid\n",
    "\"\"\"\n",
    "def columnType(column_name):\n",
    "    column_types = {\n",
    "        'name': 'NVARCHAR(80)',\n",
    "        'image': 'NVARCHAR(60)',\n",
    "        'id': 'INT',\n",
    "        'description': 'NTEXT',\n",
    "        'money': 'DECIMAL(19,4)',\n",
    "        'percentage': 'DECIMAL(12,12)',\n",
    "        'date': 'NVARCHAR(30)',\n",
    "        'code': 'NVARCHAR(40)',\n",
    "        'char': 'CHAR(1)',\n",
    "        'number': 'INT',\n",
    "        'phone': 'NVARCHAR(30)',\n",
    "        'address': 'NVARCHAR(80)',\n",
    "        'bool': 'BIT',\n",
    "    }\n",
    "\n",
    "    err = ''\n",
    "    try:\n",
    "        return column_types[column_name.rsplit('_', 1)[1]]\n",
    "    except IndexError:\n",
    "        err = \"Column name doesn't contain a type\"\n",
    "    except KeyError:\n",
    "        err = \"Column type not found\"\n",
    "    raise Exception(err)\n",
    "\n",
    "\"\"\"\n",
    "Method to insert dataframe data into SQL server\n",
    "\"\"\"\n",
    "def createTable(tablename, dataframe, PK):\n",
    "    SK = ''\n",
    "    if PK == None:\n",
    "        SK = f'SK_{tablename}'\n",
    "        columns = ''\n",
    "    else:\n",
    "        SK = f'SK_{PK}'\n",
    "        columns = f'{PK} {columnType(PK)} NOT NULL'\n",
    "    # Add Primary Key as third column\n",
    "    \n",
    "    # Add all the other columns\n",
    "    for column in dataframe.columns:\n",
    "        if column != PK: # PK is already added\n",
    "            columns += f', {column} {columnType(column)}'\n",
    "\n",
    "    surogate_columns = f\"{SK} INT IDENTITY(1,1) NOT NULL PRIMARY KEY, Timestamp DATETIME NOT NULL DEFAULT(GETDATE())\"\n",
    "\n",
    "    # Create the command\n",
    "    command = f\"CREATE TABLE {tablename} ({surogate_columns}, {columns})\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        if 'There is already an object named' in str(e):\n",
    "            print('Table already exists in database')\n",
    "        else:\n",
    "            raise(e)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method to insert dataframe data into SQL server\n",
    "\"\"\"\n",
    "def insertTable(tablename, dataframe, PK):\n",
    "    # Add Primary Key as first column\n",
    "    columns = PK\n",
    "    \n",
    "    # Add all the other columns\n",
    "    for column in dataframe.columns:\n",
    "        if column != PK: # PK is already added\n",
    "            columns += f', {column}'\n",
    "    \n",
    "    # Execute inserts\n",
    "    for i, row in dataframe.iterrows():\n",
    "        values = ''\n",
    "        values += str(row[PK])\n",
    "\n",
    "        for column in dataframe.columns:\n",
    "            if column != PK: # PK is already added\n",
    "                try:\n",
    "                    val = str(row[column]).replace(\"'\",\"''\")\n",
    "                    if val != 'None':\n",
    "                        values += f\", '{val}'\"\n",
    "                    else:\n",
    "                        values += f\", NULL\"\n",
    "                except AttributeError:\n",
    "                    values += f\", NULL\"\n",
    "\n",
    "        command = f\"INSERT INTO {tablename} ({columns}) VALUES ({values});\\n\"\n",
    "        \n",
    "        cursor.execute(command)\n",
    "    \n",
    "    try:\n",
    "        cursor.commit()\n",
    "    except pyodbc.Error as e:\n",
    "        if 'There is already an object named' in str(e):\n",
    "            print('Table already exists in database')\n",
    "        else:\n",
    "            print(command)\n",
    "            print(e)\n",
    "\n",
    "# Tables to create at end         \n",
    "etl_tables = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu gaan we eindelijk de data transformeren en in de database stoppen. Eerst gaan we producten, staff, satisfaction, course, sales_forcast, retailer_contact, retailer, Orders, returned_season, returned_item en Order_details importeren. Dit zijn de tabellen die we hebben gemaakt in de database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "### Producten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 10 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "product_etl = pd.merge(product, product_type, on=\"PRODUCT_TYPE_CODE\")\n",
    "product_etl = pd.merge(product_etl, product_line, on=\"PRODUCT_LINE_CODE\")\n",
    "\n",
    "# Hernoem\n",
    "product_etl = product_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "product_etl = filterColumns(product_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(product_etl,10)\n",
    "product_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Product', product_etl, 'PRODUCT_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales_staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 23 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "sales_staff_etl = pd.merge(sales_staff, sales_branch, on='SALES_BRANCH_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, country, on='COUNTRY_CODE')\n",
    "sales_staff_etl = pd.merge(sales_staff_etl, sales_territory, on='SALES_TERRITORY_CODE')\n",
    "\n",
    "# Hernoem\n",
    "sales_staff_etl = sales_staff_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "sales_staff_etl = filterColumns(sales_staff_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_staff_etl,23)\n",
    "sales_staff_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Sales_Staff', sales_staff_etl, 'SALES_STAFF_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisfaction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "satisfaction_type_etl = satisfaction_type.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "satisfaction_type_etl = filterColumns(satisfaction_type_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(satisfaction_type_etl,2)\n",
    "satisfaction_type_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Satisfaction_Type', satisfaction_type_etl, 'SATISFACTION_TYPE_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "course_etl = course.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "course_etl = filterColumns(course_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(course_etl,2)\n",
    "course_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Course', course_etl, 'COURSE_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 4 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "sales_forecast_etl = sales_forecast.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "sales_forecast_etl = filterColumns(sales_forecast_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_forecast_etl,4)\n",
    "sales_forecast_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Sales_Forecast', sales_forecast_etl, 'PRODUCT_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retailer_contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 23 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_contact_etl = pd.merge(retailer_contact, retailer_site, on='RETAILER_SITE_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, country, on='COUNTRY_CODE')\n",
    "retailer_contact_etl = pd.merge(retailer_contact_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    \n",
    "# Hernoem \n",
    "retailer_contact_etl = retailer_contact_etl.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "retailer_contact_etl = filterColumns(retailer_contact_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(retailer_contact_etl,23)\n",
    "retailer_contact_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Retailer_Contact', retailer_contact_etl, 'RETAILER_CONTACT_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 22 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "retailer_etl = pd.merge(retailer, retailer_headquarters, on='RETAILER_CODEMR')\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_type, on='RETAILER_TYPE_CODE')\n",
    "\n",
    "# Merge en hernoem de taal kolommen via de country tabel en retailer_segment tabel\n",
    "retailer_etl = pd.merge(retailer_etl, retailer_segment, on='SEGMENT_CODE').rename(columns={'LANGUAGE':'SEGMENT_LANGUAGE_code'})\n",
    "retailer_etl = pd.merge(retailer_etl, country, on='COUNTRY_CODE').rename(columns={'LANGUAGE':'COUNTRY_LANGUAGE_code'})\n",
    "\n",
    "# Sluit kolommen vroegtijdig uit vanwege samenvoegingsnaamconflicten, want duidelijk creert SQL Server deze kolommen.\n",
    "retailer_etl = excludeColumns(retailer_etl, ['TRIAL219','TRIAL222_x','TRIAL222_y','TRIAL222'])\n",
    "\n",
    "# Hernoem\n",
    "retailer_etl = pd.merge(retailer_etl, sales_territory, on='SALES_TERRITORY_CODE')\\\n",
    "    .rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "retailer_etl = filterColumns(retailer_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(retailer_etl,22)\n",
    "retailer_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Retailer', retailer_etl, 'RETAILER_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Merge\n",
    "order_etl = pd.merge(order_header, order_method, on='ORDER_METHOD_CODE').rename(columns=json_file)\n",
    "\n",
    "# Sluit redundante kolommen met externe sleutels uit\n",
    "# RETAILER_SITE_code word afgeleid van RETAILER_CONTACT_id\n",
    "# SALES_BRANCH_code word afgeleid van SALES_STAFF_id\n",
    "order_etl = excludeColumns(order_etl, ['RETAILER_SITE_id', 'SALES_BRANCH_id'])\n",
    "\n",
    "order_etl.reset_index(inplace=True)\n",
    "order_etl.rename(columns={'index': 'SURROGATE_KEY'}, inplace=True)\n",
    "\n",
    "# Filter\n",
    "order_etl = filterColumns(order_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(order_etl,7)\n",
    "order_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Orders', order_etl, 'ORDER_TABLE_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 2 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "return_reason_etl = return_reason.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "return_reason_etl = filterColumns(return_reason_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(return_reason_etl,2)\n",
    "return_reason_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Return_Reason', return_reason_etl, 'RETURN_REASON_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem \n",
    "returned_item_etl = returned_item.rename(columns=json_file)\n",
    "\n",
    "# Filter \n",
    "returned_item_etl = filterColumns(returned_item_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(returned_item_etl,5)\n",
    "returned_item_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Returns', returned_item_etl, 'RETURNS_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 7 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "order_detail_etl = order_details.rename(columns=json_file)\n",
    "\n",
    "# Filter\n",
    "order_detail_etl = filterColumns(order_detail_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(order_detail_etl,7)\n",
    "order_detail_etl\n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Order_Details', order_detail_etl, 'ORDER_DETAIL_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has 5 columns\n"
     ]
    }
   ],
   "source": [
    "# Hernoem\n",
    "sales_target_etl = SALES_TARGETData.rename(columns=json_file)\n",
    "sales_target_etl = sales_target_etl.rename(columns={'Id':'TARGET_id'})\n",
    "\n",
    "# Filter\n",
    "sales_target_etl = filterColumns(sales_target_etl)\n",
    "\n",
    "# Check\n",
    "sizeCheck(sales_target_etl,5)\n",
    "sales_target_etl  \n",
    "\n",
    "# Create Table en doe het in de lijst.\n",
    "etl_tables.append(('Sales_Target', sales_target_etl, 'TARGET_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploaden naar de database.\n",
    "\n",
    "Nu gaan we alle data uploaden naar de database om ervoor te zorgen dat we alle data juist in de database hebben geupload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Product\n",
      "Inserted Product\n",
      "Creating Sales_Staff\n",
      "Inserted Sales_Staff\n",
      "Creating Satisfaction_Type\n",
      "Inserted Satisfaction_Type\n",
      "Creating Course\n",
      "Inserted Course\n",
      "Creating Sales_Forecast\n",
      "Inserted Sales_Forecast\n",
      "Creating Retailer_Contact\n",
      "Inserted Retailer_Contact\n",
      "Creating Retailer\n",
      "Inserted Retailer\n",
      "Creating Orders\n",
      "Inserted Orders\n",
      "Creating Return_Reason\n",
      "Inserted Return_Reason\n",
      "Creating Returns\n",
      "Inserted Returns\n",
      "Creating Order_Details\n",
      "Inserted Order_Details\n",
      "Creating Sales_Target\n",
      "Inserted Sales_Target\n",
      "All is done\n"
     ]
    }
   ],
   "source": [
    "# Nu maken we de tabellen aan\n",
    "for table in etl_tables:\n",
    "    print(f\"Creating {table[0]}\")\n",
    "    createTable(table[0], table[1], table[2])\n",
    "    insertTable(table[0], table[1], table[2])\n",
    "    print(f\"Inserted {table[0]}\")\n",
    "\n",
    "# Close the connection\n",
    "print(\"All is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder gaan we de data inladen vanuit de SQL Server database. Met de database verzorgen we ervoor dat we makkelijk de data kunnen inladen in de database. We maken een functie die de data inlaad in de database. Dat deden we all gedeelte lijk boven, maar nu gaan we de data uit de database halen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "Sales_Staff\n",
      "Satisfaction_Type\n",
      "Course\n",
      "Sales_Forecast\n",
      "Retailer_Contact\n",
      "Retailer\n",
      "Orders\n",
      "Return_Reason\n",
      "Returns\n",
      "Order_Details\n",
      "Sales_Target\n",
      "\n",
      "Resultaat:\n",
      "[(94, datetime.datetime(2024, 3, 18, 10, 21, 51, 620000), 8, 'The TrailChef Double Flame is a camp stove which attaches directly to fuel canisters. Features precise flame control and a built-in windscreen.  Weight is 850 g, fuel is propane.', 'P08CE1CG1.jpg', Decimal('0.410000000000'), Decimal('75.0000'), 1, 'Camping Equipment', '5-3-2013', 'EN', 'TrailChef Double Flame')]\n"
     ]
    }
   ],
   "source": [
    "tables = cursor.execute(\"SELECT t.name FROM sys.tables t\")\n",
    "tables = tables.fetchall()\n",
    "for table in tables:\n",
    "    table = table[0]\n",
    "    print(table)\n",
    "print()\n",
    "\n",
    "print(\"Resultaat:\")\n",
    "leef = cursor.execute(\"SELECT * FROM PRODUCT WHERE PRODUCT_id = '8'\")\n",
    "print(leef.fetchall())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "warehouse-YJ77YNfe-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
